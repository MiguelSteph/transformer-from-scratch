{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d34bc1b5",
   "metadata": {},
   "source": [
    "# Install and import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f18be33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.5/494.5 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.7/76.7 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q -U grain clu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8798e1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jax Device count: 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"]=\"false\"\n",
    "os.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"]=\"platform\"\n",
    "\n",
    "'''\n",
    "    Uncomment the line bellow to simulate 8 devices when running on CPU.\n",
    "'''\n",
    "# flags = os.environ.get(\"XLA_FLAGS\", \"\")\n",
    "# flags += \" --xla_force_host_platform_device_count=8\"  # Simulate 8 devices\n",
    "# os.environ[\"XLA_FLAGS\"] = flags\n",
    "\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "import flax\n",
    "from flax.training.common_utils import shard, shard_prng_key\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "from clu import metric_writers\n",
    "import orbax.checkpoint as ocp\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "import nltk\n",
    "\n",
    "prng_main_key = jax.random.key(18)\n",
    "\n",
    "print(f\"Jax Device count: {jax.device_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47644a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import custom files\n",
    "\n",
    "# Download the code to the current working directory\n",
    "REPO_NAME = 'transformer-from-scratch'\n",
    "TRAINING_OUTPUT_PATH = 'transformer-from-scratch/training_output/'\n",
    "if not os.path.isdir(REPO_NAME):\n",
    "    !git clone https://github.com/MiguelSteph/transformer-from-scratch.git\n",
    "\n",
    "%run 'transformer-from-scratch/configs/configs.py'\n",
    "%run 'transformer-from-scratch/dataset_utils.py'\n",
    "%run 'transformer-from-scratch/tokenizer.py'\n",
    "%run 'transformer-from-scratch/models.py'\n",
    "%run 'transformer-from-scratch/train_and_eval_utils.py'\n",
    "\n",
    "\n",
    "config = get_configs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd51aa9",
   "metadata": {},
   "source": [
    "# Download the dataset\n",
    "We will use the french to english dataset.\n",
    "It can be downloaded at https://storage.googleapis.com/download.tensorflow.org/data/fra-eng.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d72f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_data(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bed2473",
   "metadata": {},
   "source": [
    "# Build or Load BPE tokenizer\n",
    "In the first version of the project, I used the tiktoken tokenizer. Since we have a small dataset, I noticed the the number of token is too much and the model has some issue to increase its accuracy.\n",
    "I switched to creating a custom tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87550334",
   "metadata": {},
   "outputs": [],
   "source": [
    "trg_tokenizer_path = TRAINING_OUTPUT_PATH + config.trg_tokenizer_path\n",
    "src_tokenizer_path = TRAINING_OUTPUT_PATH + config.src_tokenizer_path\n",
    "\n",
    "if not os.path.exists(TRAINING_OUTPUT_PATH + config.trg_tokenizer_path + '/vocab.json'):\n",
    "    build_and_save_tokenizer(config)\n",
    "    trg_tokenizer_path = config.trg_tokenizer_path\n",
    "    src_tokenizer_path = config.src_tokenizer_path\n",
    "else:\n",
    "    print(\"Tokenizer is already built.\")\n",
    "\n",
    "# Load saved tokenizer\n",
    "fr_tokenizer = ByteLevelBPETokenizer(trg_tokenizer_path + '/vocab.json', trg_tokenizer_path + '/merges.txt')\n",
    "en_tokenizer = ByteLevelBPETokenizer(src_tokenizer_path + '/vocab.json', src_tokenizer_path + '/merges.txt')\n",
    "fr_tokenizer.add_special_tokens(config.tokenizer_special_tokens)\n",
    "en_tokenizer.add_special_tokens(config.tokenizer_special_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e463f4fe",
   "metadata": {},
   "source": [
    "# Data exploration\n",
    "We will explore the following aspect of the dataset:\n",
    "* overall number of sample in the dataset\n",
    "* Number of unique sentences both in english and french\n",
    "* Number of tokens distribution both in english and french\n",
    "\n",
    "The exploration has the following two goals:\n",
    "* Determine the correct max sequence length for the model\n",
    "* Better slit the dataset into train, validation and test sets. We don't want to include the same sentences into training and validation/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0065468",
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_sentences = []\n",
    "en_sentences = []\n",
    "\n",
    "with open(config.dataset_path) as f:\n",
    "    lines = f.read().split('\\n')\n",
    "    for line in lines:\n",
    "        line_split = line.split('\\t')\n",
    "        if len(line_split) == 2:\n",
    "            fr_sentences.append(line_split[1].strip())\n",
    "            en_sentences.append(line_split[0].strip())\n",
    "\n",
    "dataset_size = len(en_sentences)\n",
    "distinct_fr_sentences = len(set(fr_sentences))\n",
    "distinct_en_sentences = len(set(en_sentences))\n",
    "\n",
    "fr_distinct_percentage = (distinct_fr_sentences * 100) // dataset_size\n",
    "en_distinct_percentage = (distinct_en_sentences * 100) // dataset_size\n",
    "\n",
    "\n",
    "print(f\"Dataset size is {dataset_size}\")\n",
    "print(f\"Number of unique french sentences is {distinct_fr_sentences} which is {fr_distinct_percentage}%\")\n",
    "print(f\"Number of unique english sentences is {distinct_en_sentences} which is {en_distinct_percentage}%\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Token distribution\n",
    "fr_tokens_len = [len(fr_tokenizer.encode(sentence).ids) for sentence in fr_sentences]\n",
    "en_tokens_len = [len(en_tokenizer.encode(sentence).ids) for sentence in en_sentences]\n",
    "\n",
    "print(f\"Max token length for french sentences is {max(fr_tokens_len)}\")\n",
    "print(f\"Max token length for english sentences is {max(en_tokens_len)}\")\n",
    "\n",
    "plt.hist([en_tokens_len, fr_tokens_len], color=['r', 'b'], label=['english', 'french'], alpha=0.5)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "del fr_sentences\n",
    "del en_sentences\n",
    "del distinct_fr_sentences\n",
    "del distinct_en_sentences\n",
    "del dataset_size\n",
    "del fr_distinct_percentage\n",
    "del en_distinct_percentage\n",
    "del fr_tokens_len\n",
    "del en_tokens_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85a21b3",
   "metadata": {},
   "source": [
    "**Takeaways:**\n",
    "* only 69% of the english sentences are distincts. We need to be careful when creating the validation and test sets.\n",
    "* We will set the max token length for french sentences to **120**.\n",
    "* We will set the max token length for english sentences to **60**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530a905a",
   "metadata": {},
   "source": [
    "# Dataset preparation\n",
    "Here we will split the dataset into three sets: training, validation and test set. The test and validation set will represent each **10%** of the unique sentences and the training set will represent the remaining data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65c5aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "prng_main_key, data_preparation_prng = jax.random.split(prng_main_key, 2)\n",
    "train_loader, val_loader, test_loader = load_and_prepare_dataset(en_tokenizer, \n",
    "                                                                 fr_tokenizer,\n",
    "                                                                 config.dataset_path, \n",
    "                                                                 data_preparation_prng, \n",
    "                                                                 num_epochs=config.training_epochs, \n",
    "                                                                 train_batch_size=config.batch_size, \n",
    "                                                                 test_or_val_batch_size=config.batch_size,\n",
    "                                                                 max_src_len=config.max_src_len,\n",
    "                                                                 max_trg_len=config.max_trg_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b1c32a",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "For this tutorial, we want to implement from scratch the [original transformer model](https://arxiv.org/abs/1706.03762) with Jax.\n",
    "<!-- ![Model architecture](https://arxiv.org/html/1706.03762v7/Figures/ModalNet-21.png) -->\n",
    "\n",
    "In this tutorial, we will focus on the implementation part. We will start our journey with the positional encoding component.\n",
    "\n",
    "## Positional encoding\n",
    "The positional encoding module is the module that add encode the position of the word in the model. Same as the original paper, we will use a static positonal encoding computed based on the following formular:\n",
    "\n",
    "***TODO: Add an image of the formular.***\n",
    "\n",
    "## Multi-head attention module\n",
    "\n",
    "The central piece of the multi-heat attention layer is the scaled dot product attention.\n",
    "\n",
    "## Feed-forward component\n",
    "The Feed forward component consists of two linear transformations with a ReLU activation function in between. We will create a module for that as well.\n",
    "\n",
    "## Add&Norm module\n",
    "This layer takes as input the output of a sub layer and a residual connection. It applies a dropout to the output of the previous sublayer and add that to the residual connection. It finally normalizes the result. We will a layer normalization module here instead of the batch naormalization layer used in the original paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da0467b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing model components\n",
    "\n",
    "test_num_heads = 8\n",
    "test_d_proj = 32\n",
    "test_use_causal_mask= True\n",
    "test_emb_dim = 256\n",
    "test_src_len= 60\n",
    "test_batch_size = 32\n",
    "test_d_inner = 1024\n",
    "test_dropout = 0.2\n",
    "test_num_blocks = 6\n",
    "test_ff_d_inner = 512\n",
    "test_max_vocab_size = 5000\n",
    "max_seq_len = 64\n",
    "\n",
    "test_prng_key = jax.random.key(44)\n",
    "key_1, key_2, key_3, key_4, key_5, dropout_key = jax.random.split(test_prng_key, 6)\n",
    "\n",
    "\n",
    "# Test the positional encoding module\n",
    "test_random_input = jax.random.normal(test_prng_key, (config.batch_size, config.max_trg_len, config.emb_dim))\n",
    "\n",
    "test_pos_enc_module = PositionalEncoding(config.emb_dim, config.max_trg_len)\n",
    "variables = test_pos_enc_module.init(test_prng_key, test_random_input)\n",
    "pos_output = test_pos_enc_module.apply({}, test_random_input)\n",
    "\n",
    "# Expect the input shape to be the same as the output shape\n",
    "print(\"Testing positional encoding module\")\n",
    "print(f\"Input shape: {test_random_input.shape}\")\n",
    "print(f\"Output shape: {pos_output.shape}\")\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Test multi head attention layer shape output\n",
    "test_multi_head_att_module = MultiHeadAttentionModule(test_num_heads, \n",
    "                                                      test_emb_dim, \n",
    "                                                      test_d_proj, \n",
    "                                                      test_d_proj, \n",
    "                                                      use_causal_mask=True)\n",
    "k = jax.random.normal(key_1, (test_batch_size, test_src_len, test_emb_dim))\n",
    "v = jax.random.normal(key_2, (test_batch_size, test_src_len, test_emb_dim))\n",
    "q = jax.random.normal(key_3, (test_batch_size, test_src_len, test_emb_dim))\n",
    "sample_mask = jax.random.choice(key_3, 2, (test_batch_size, test_src_len))\n",
    "\n",
    "variables = test_multi_head_att_module.init(test_prng_key, k, v, q)\n",
    "params = variables['params']\n",
    "\n",
    "attentions = test_multi_head_att_module.apply({'params': params}, k, v, q, mask=sample_mask)\n",
    "print(\"Testing multi head attention module\")\n",
    "print(f\"Expected shape: {(test_batch_size, test_src_len, test_emb_dim)}\")\n",
    "print(f\"Actual shape: {attentions.shape}\")\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Test Feedforward module\n",
    "sample_input = jax.random.normal(key_1, (test_batch_size, test_src_len, test_emb_dim))\n",
    "\n",
    "test_ff_module = FeedForwardModule(test_d_inner, test_emb_dim)\n",
    "variables = test_ff_module.init(key_2, sample_input)\n",
    "\n",
    "test_output = test_ff_module.apply(variables, sample_input)\n",
    "print(\"Testing Feedforward module\")\n",
    "print(f\"Expected output shape: {(test_batch_size, test_src_len, test_emb_dim)}\")\n",
    "print(f\"Actual output shape: {test_output.shape}\")\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Test Add&Norm module\n",
    "sample_x = jax.random.normal(key_1, (test_batch_size, test_src_len, test_emb_dim))\n",
    "sample_residual_x = jax.random.normal(key_2, (test_batch_size, test_src_len, test_emb_dim))\n",
    "\n",
    "test_add_norm_module = AddAndNormModule(test_dropout)\n",
    "variables = test_add_norm_module.init(key_3, sample_x, sample_residual_x, training=True)\n",
    "\n",
    "test_output = test_add_norm_module.apply(variables, \n",
    "                                         sample_x, \n",
    "                                         sample_residual_x, \n",
    "                                         training=True, \n",
    "                                         rngs={'dropout': dropout_key})\n",
    "print(\"Testing Add&Norm module\")\n",
    "print(f\"Expected output shape: {(test_batch_size, test_src_len, test_emb_dim)}\")\n",
    "print(f\"Actual output shape: {test_output.shape}\")\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Test Encoder block module \n",
    "sample_mask = jax.random.choice(key_1, 2, (test_batch_size, test_src_len))\n",
    "sample_x = jax.random.normal(key_2, (test_batch_size, test_src_len, test_emb_dim))\n",
    "\n",
    "test_encoder_block_module = EncoderBlockModule(test_d_inner, test_emb_dim, \n",
    "                                               test_dropout, test_num_heads, test_d_proj)\n",
    "variables = test_encoder_block_module.init(key_3, sample_x)\n",
    "test_output = test_encoder_block_module.apply(variables, sample_x, \n",
    "                                              mask=sample_mask, training=True, \n",
    "                                              rngs={'dropout': dropout_key})\n",
    "print(\"Testing Encoder block module\")\n",
    "print(f\"Expected output shape: {(test_batch_size, test_src_len, test_emb_dim)}\")\n",
    "print(f\"Actual output shape: {test_output.shape}\")\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Test Decoder block module \n",
    "sample_mask = jax.random.choice(key_1, 2, (test_batch_size, test_src_len))\n",
    "sample_x = jax.random.normal(key_2, (test_batch_size, test_src_len, test_emb_dim))\n",
    "sample_enc_output = jax.random.normal(key_3, (test_batch_size, test_src_len, test_emb_dim))\n",
    "\n",
    "test_decoder_block_module = DecoderBlockModule(test_d_inner, test_emb_dim, \n",
    "                                               test_dropout, test_num_heads, test_d_proj)\n",
    "variables = test_decoder_block_module.init(key_4, sample_x, sample_enc_output)\n",
    "test_output = test_decoder_block_module.apply(variables, sample_x, sample_enc_output, \n",
    "                                              mask=sample_mask, training=True, \n",
    "                                              rngs={'dropout': dropout_key})\n",
    "print(\"Testing Decoder block module\")\n",
    "print(f\"Expected output shape: {(test_batch_size, test_src_len, test_emb_dim)}\")\n",
    "print(f\"Actual output shape: {test_output.shape}\")\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Test transformer output shape\n",
    "sample_enc_mask = jax.random.choice(key_1, 2, (test_batch_size, test_src_len))\n",
    "sample_dec_mask = jax.random.choice(key_2, 2, (test_batch_size, test_src_len))\n",
    "sample_enc_x = jax.random.choice(key_3, test_max_vocab_size, (test_batch_size, test_src_len))\n",
    "sample_dec_x = jax.random.choice(key_4, test_max_vocab_size, (test_batch_size, test_src_len))\n",
    "\n",
    "test_transformer_module = TransformerModule(test_num_blocks, test_ff_d_inner, \n",
    "                                            test_emb_dim, test_dropout, \n",
    "                                            test_num_heads, test_d_proj, \n",
    "                                            test_max_vocab_size, max_seq_len)\n",
    "variables = test_transformer_module.init(key_5, sample_enc_x, sample_dec_x)\n",
    "test_output = test_transformer_module.apply(variables, sample_enc_x, sample_dec_x, \n",
    "                                            sample_enc_mask, sample_dec_mask, \n",
    "                                            training=True, rngs={'dropout': dropout_key})\n",
    "\n",
    "print(\"Testing Full transformer module\")\n",
    "print(f\"Expected output shape: {(test_batch_size, test_src_len, test_max_vocab_size)}\")\n",
    "print(f\"Actual output shape: {test_output.shape}\")\n",
    "\n",
    "\n",
    "del test_random_input\n",
    "del test_pos_enc_module\n",
    "del pos_output\n",
    "del sample_mask\n",
    "del sample_input\n",
    "del sample_x\n",
    "del sample_residual_x\n",
    "del test_ff_module\n",
    "del sample_enc_output\n",
    "del test_encoder_block_module\n",
    "del test_decoder_block_module\n",
    "del test_multi_head_att_module\n",
    "del test_add_norm_module\n",
    "del sample_enc_mask\n",
    "del sample_dec_mask\n",
    "del sample_enc_x\n",
    "del sample_dec_x\n",
    "del test_transformer_module\n",
    "del k\n",
    "del v\n",
    "del q\n",
    "del params\n",
    "del attentions\n",
    "del variables\n",
    "del test_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5383d5",
   "metadata": {},
   "source": [
    "# Training\n",
    "## Visualize learning rate\n",
    "We have a learning rate decay function that slowly warm up the learning rate to a base learning rate and then applies cosine decay to it for the rest of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1259e692",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_lr=0.01\n",
    "warmup_epochs=2\n",
    "cosine_epochs=8\n",
    "steps_per_epochs=1000\n",
    "\n",
    "steps = list(range(10000))\n",
    "scheduler = create_learning_rate_scheduler(base_lr, warmup_epochs, \n",
    "                                           cosine_epochs, steps_per_epochs)\n",
    "lr_val = [scheduler(step) for step in range(10000)]\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(12, 4))\n",
    "ax.plot(steps, lr_val)\n",
    "ax.grid()\n",
    "ax.set(xlabel='Steps', ylabel='Learning rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07687cf0",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9477bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_train_step = jax.pmap(train_step, axis_name=\"batch\", donate_argnums=(0,))\n",
    "parallel_eval_step = jax.pmap(eval_step, axis_name=\"batch\")\n",
    "prng_main_key, param_init_prng_key, dropout_key = jax.random.split(prng_main_key, 3)\n",
    "\n",
    "def train_and_evaluate(param_init_prng_key, dropout_key, config, train_loader, val_loader):\n",
    "    writer = metric_writers.create_default_writer(config.metric_path)\n",
    "    ckp_options = ocp.CheckpointManagerOptions(max_to_keep=3, \n",
    "                                               save_interval_steps=1, \n",
    "                                               best_fn= lambda metrics: metrics['acc'])\n",
    "    ckp_mngr = ocp.CheckpointManager(os.path.abspath(config.checkpoint_path), options=ckp_options)\n",
    "\n",
    "    train_loader_iter = iter(train_loader)\n",
    "    state = create_train_state(config, param_init_prng_key, dropout_key)\n",
    "\n",
    "    # Replicate the train state for pmap\n",
    "    state = flax.jax_utils.replicate(state)\n",
    "    state = state.replace(dropout_key=shard_prng_key(dropout_key))\n",
    "    \n",
    "    for epoch in range(config.training_epochs):\n",
    "        print(f\"\\nTraining Epoch {epoch + 1}\\n\")\n",
    "        acc_vals = jnp.array([])\n",
    "        loss_vals = jnp.array([])\n",
    "        with tqdm(total=config.steps_per_epochs, desc=f\"Training Epoch {epoch}\", leave=False) as progress_bar_train:\n",
    "            for step in tqdm(range(config.steps_per_epochs)):\n",
    "                train_batch = next(train_loader_iter)\n",
    "                train_batch = shard(train_batch)\n",
    "                \n",
    "                state, batch_metrics = parallel_train_step(state, train_batch)\n",
    "                acc_vals = jnp.concatenate([acc_vals, batch_metrics['acc'][:1]])\n",
    "                loss_vals = jnp.concatenate([loss_vals, batch_metrics['loss'][:1]])\n",
    "                progress_bar_train.update(1)\n",
    "\n",
    "            for step in range(config.steps_per_epochs):\n",
    "                writer.write_scalars(step, {'acc': acc_vals[step], 'loss': loss_vals[step]})\n",
    "            progress_bar_train.write(f\"Training Epoch {epoch} End => accuracy: {batch_metrics['acc'][0]}    Loss: {batch_metrics['loss'][0]}\")\n",
    "\n",
    "        print(f\"\\nEval epoch {epoch + 1}\")\n",
    "        eval_acc_vals = jnp.array([])\n",
    "        eval_loss_vals = jnp.array([])\n",
    "        val_loader_iter = iter(val_loader)\n",
    "        with tqdm(total=config.val_steps, desc=f\"Validation Epoch {epoch}\", leave=False) as progress_bar_validation:\n",
    "            for val_step in tqdm(range(config.val_steps)):\n",
    "                val_batch = next(val_loader_iter)\n",
    "                val_batch = shard(val_batch)\n",
    "                \n",
    "                val_metrics = parallel_eval_step(state, val_batch)\n",
    "                eval_acc_vals = jnp.concatenate([eval_acc_vals, val_metrics['acc'][:1]])\n",
    "                eval_loss_vals = jnp.concatenate([eval_loss_vals, val_metrics['loss'][:1]])\n",
    "                progress_bar_validation.update(1)\n",
    "            progress_bar_validation.write(f\"Validation Epoch {epoch + 1} => accuracy: {jnp.mean(eval_acc_vals)}    Loss: {jnp.mean(eval_loss_vals)}\")\n",
    "\n",
    "        ckp_mngr.save(epoch,\n",
    "              args=ocp.args.StandardSave(flax.jax_utils.unreplicate(state)),\n",
    "              metrics={'acc': float(jnp.mean(eval_acc_vals)), 'loss': float(jnp.mean(eval_loss_vals))})\n",
    "    \n",
    "    ckp_mngr.wait_until_finished()\n",
    "    \n",
    "    return state\n",
    "\n",
    "\n",
    "state = train_and_evaluate(param_init_prng_key, dropout_key, config, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6243b3",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b802e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fr_tokenizer = ByteLevelBPETokenizer(config.trg_tokenizer_path + '/vocab.json', config.trg_tokenizer_path + '/merges.txt')\n",
    "# fr_tokenizer.add_special_tokens(config.tokenizer_special_tokens)\n",
    "\n",
    "# en_tokenizer = ByteLevelBPETokenizer(config.src_tokenizer_path + '/vocab.json', config.src_tokenizer_path + '/merges.txt')\n",
    "# en_tokenizer.add_special_tokens(config.tokenizer_special_tokens)\n",
    "\n",
    "ckp_path = TRAINING_OUTPUT_PATH + config.checkpoint_path\n",
    "\n",
    "model = create_model(config)\n",
    "ckp_options = ocp.CheckpointManagerOptions(max_to_keep=3, \n",
    "                                       save_interval_steps=1, \n",
    "                                       best_fn= lambda metrics: metrics['acc'])\n",
    "ckp_mngr = ocp.CheckpointManager(os.path.abspath(ckp_path), options=ckp_options)\n",
    "restored_state = ckp_mngr.restore(step=None, args=ocp.args.StandardRestore())\n",
    "\n",
    "# Test\n",
    "src_sentence = 'Thank God'\n",
    "translation_text = run_inference(src_sentence, config, model, restored_state['params'], en_tokenizer, fr_tokenizer)\n",
    "print(translation_text)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ecc733",
   "metadata": {},
   "source": [
    "## Bleu score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6852b812",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_sentences = []\n",
    "fr_translations = []\n",
    "for sample in test_loader:\n",
    "    for idx in range(sample['src_tokens'].shape[0]):\n",
    "        en_sentence = en_tokenizer.decode(sample['src_tokens'][idx])\n",
    "        fr_translation = fr_tokenizer.decode(sample['trg_input_tokens'][idx])\n",
    "        en_sentences.append(en_sentence)\n",
    "        fr_translations.append(fr_translation)\n",
    "\n",
    "# Randomnly pick 200 sentences for BLEU score computation\n",
    "indices = jax.random.choice(prng_main_key, len(en_sentences), (200,))\n",
    "bleu_en_sentences = [en_sentences[x] for x in indices]\n",
    "bleu_fr_original_transaltion = [fr_translations[x] for x in indices]\n",
    "model_translations = []\n",
    "for idx in range(200):\n",
    "    translation = run_inference(bleu_en_sentences[idx], config, model, restored_state['params'], en_tokenizer, fr_tokenizer)\n",
    "    model_translations.append(translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4c708f",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = [x.split() for x in model_translations]\n",
    "references = [x.split() for x in bleu_fr_original_transaltion]\n",
    "bleu_scores = []\n",
    "\n",
    "\n",
    "for idx in range(200):\n",
    "    score = nltk.translate.bleu_score.sentence_bleu([references[idx]], candidates[idx])\n",
    "    bleu_scores.append(score)\n",
    "bleu_score = sum(bleu_scores) / len(bleu_scores)\n",
    "print(f\"BLEU Score: {bleu_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe3eacb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
